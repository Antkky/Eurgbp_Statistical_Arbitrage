{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gym & Gym_Anytrading\n",
    "import gym\n",
    "import gym.vector\n",
    "import gym_anytrading\n",
    "\n",
    "# Pytorch Modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "# Data Structures\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "\n",
    "# Misc\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Stuff\n",
    "\n",
    "### Import data using Pandas\n",
    "```python\n",
    "eData = pd.read_csv(\"data/EURUSDp.csv\")\n",
    "...\n",
    "```\n",
    "\n",
    "### Format datetime\n",
    "```python\n",
    "eData[\"Gmt time\"] = pd.to_datetime(eData[\"Gmt time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "...\n",
    "```\n",
    "\n",
    "### Set Index\n",
    "```python\n",
    "eData.set_index(\"Gmt Time\", inplace=True)\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eData = pd.read_csv(\"data/EURUSDp.csv\") # make sure these have correct column names\n",
    "pData = pd.read_csv(\"data/GBPUSDp.csv\") # make sure these have correct column names\n",
    "xData = pd.read_csv(\"data/processed/EXTRAp.csv\") # make sure these have correct column names\n",
    "\n",
    "eData[\"Gmt time\"] = pd.to_datetime(eData[\"Gmt time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "pData[\"Gmt time\"] = pd.to_datetime(pData[\"Gmt time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "xData[\"Gmt time\"] = pd.to_datetime(xData[\"Gmt time\"], format=\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "eData.set_index(\"Gmt Time\", inplace=True)\n",
    "pData.set_index(\"Gmt Time\", inplace=True)\n",
    "xData.set_index(\"Gmt Time\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward_function():\n",
    "  pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamic Enviroment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_feature_positioning(history):\n",
    "  return history\n",
    "\n",
    "def dynamic_feature_unrealized(history):\n",
    "  return history\n",
    "\n",
    "def dynamic_feature_realized(history):\n",
    "  return history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### make_function(n: int)\n",
    "takes in an ID number `n` then return an environment with the correct information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_function(n: int):\n",
    "  match n:\n",
    "    case 1:\n",
    "      return gym.make(\n",
    "        id=\"forex-v0\",\n",
    "        name=\"EUR/USD\",\n",
    "        df=eData,\n",
    "        dynamic_feature_functions = [],\n",
    "        reward_function=reward_function,\n",
    "        verbose=True\n",
    "       )\n",
    "    case 2:\n",
    "      return gym.make(\n",
    "        id=\"forex-v0\",\n",
    "        name=\"EUR/USD\",\n",
    "        df=pData,\n",
    "        dynamic_feature_functions = [],\n",
    "        reward_function=reward_function,\n",
    "        verbose=True\n",
    "       )\n",
    "    case 3:\n",
    "      return gym.make(\n",
    "        id=\"forex-v0\",\n",
    "        name=\"EXTRA\",\n",
    "        df=xData,\n",
    "        dynamic_feature_functions = [],\n",
    "        reward_function=reward_function,\n",
    "        verbose=True\n",
    "       )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Deep Q Learning Model\n",
    "Pytorch LSTM Model for Deep Q Learning\n",
    "\n",
    "## Constructor\n",
    "```python\n",
    "LSTM_Q_Net(input_size: int, hidden_size: int, output_size: int)\n",
    "```\n",
    "### Inputs\n",
    "- `input_size`: Number of input features\n",
    "- `hidden_size`: Number of input features\n",
    "- `output_size`: Number of input features\n",
    "\n",
    "## Overview\n",
    "```python\n",
    "forward(x: torch.tensor)\n",
    "\n",
    "save(file_name: str)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_Q_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(LSTM_Q_Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "      out, hidden = self.lstm(x, hidden)\n",
    "      out = self.fc(out[:, -1, :])\n",
    "      return out, hidden\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        model_folder_path = \"./models\"\n",
    "        os.makedirs(model_folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingAgent():\n",
    "    def __init__(self, envs, hidden_size=128, learning_rate=1e-3,\n",
    "                 gamma=0.99, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995,\n",
    "                 seq_len=32, batch_size=64, num_layers=1, memory_size=10000):\n",
    "\n",
    "        self.envs = envs\n",
    "        self.input_size = np.prod(self.envs.observation_space.shape)\n",
    "        self.output_size = self.envs.action_space.n\n",
    "\n",
    "        # Model parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Initialize models\n",
    "        self.model = LSTM_Q_Net(self.input_size, hidden_size, self.output_size).to(self.device)\n",
    "        self.target_model = LSTM_Q_Net(self.input_size, hidden_size, self.output_size).to(self.device)\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "        # Training parameters\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.scaler = torch.amp.GradScaler(device=self.device.type)\n",
    "\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "\n",
    "        # Memory and sequence parameters\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        self.seq_len = seq_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def train(self, num_episodes=1000, target_update_frequency=10, checkpoint_frequency=100,\n",
    "              render=False, checkpoint_path=\"model_checkpoints\"):\n",
    "        \"\"\"\n",
    "        Train the agent for a specified number of episodes\n",
    "\n",
    "        Args:\n",
    "            num_episodes: Number of episodes to train for\n",
    "            target_update_frequency: How often to update the target network\n",
    "            checkpoint_frequency: How often to save model checkpoints\n",
    "            render: Whether to render the environment\n",
    "            checkpoint_path: Where to save model checkpoints\n",
    "        \"\"\"\n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(1, num_episodes + 1):\n",
    "            episode_reward = self.run_episode(render)\n",
    "            total_rewards.append(episode_reward)\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % target_update_frequency == 0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            # Save checkpoint periodically\n",
    "            if episode % checkpoint_frequency == 0:\n",
    "                import os\n",
    "                os.makedirs(checkpoint_path, exist_ok=True)\n",
    "                torch.save({\n",
    "                    'episode': episode,\n",
    "                    'model_state_dict': self.model.state_dict(),\n",
    "                    'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                    'reward': episode_reward,\n",
    "                    'epsilon': self.epsilon\n",
    "                }, f\"{checkpoint_path}/checkpoint_{episode}.pt\")\n",
    "\n",
    "            # Print progress\n",
    "            self.envs.save_for_render(dir = f\"render_logs_{episode}\")\n",
    "            if episode % 10 == 0:\n",
    "                avg_reward = sum(total_rewards[-10:]) / 10\n",
    "                print(f\"Episode {episode}/{num_episodes}, Average Reward (Last 10): {avg_reward:.2f}, Epsilon: {self.epsilon:.2f}\")\n",
    "\n",
    "        print(\"Training complete!\")\n",
    "        return total_rewards\n",
    "\n",
    "    def run_episode(self, render=False):\n",
    "        \"\"\"Run a single episode and return the total reward\"\"\"\n",
    "        state = self.envs.reset()\n",
    "        state0, state1, state2 = self.expand(state)\n",
    "        hidden_state = (torch.zeros(self.num_layers, 1, self.hidden_size, dtype=torch.float16).to(self.device),\n",
    "                       torch.zeros(self.num_layers, 1, self.hidden_size, dtype=torch.float16).to(self.device))\n",
    "        episode_reward = 0\n",
    "        episode_memory = deque(maxlen=self.seq_len)\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            if render:\n",
    "                self.envs.render()\n",
    "\n",
    "            state_tensor = torch.tensor(state2, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "\n",
    "            # Epsilon-greedy action selection\n",
    "            if np.random.rand() <= self.epsilon:\n",
    "                action = self.envs.action_space.sample()\n",
    "            else:\n",
    "                with torch.no_grad(), autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "                    q_values, hidden_state = self.model(state_tensor, hidden_state)\n",
    "                    action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "            # Take action and observe next state\n",
    "            next_state, reward, done, _ = self.envs.step(action)\n",
    "            next_state0, next_state1, next_state2 = self.expand(next_state)\n",
    "            episode_reward += reward\n",
    "\n",
    "            # Store experience in episode memory\n",
    "            episode_memory.append((state2, action, reward, next_state2, done, hidden_state))\n",
    "\n",
    "            # Update state\n",
    "            state0, state1, state2 = next_state0, next_state1, next_state2\n",
    "\n",
    "            # Train on episode memory if enough experience is collected or episode is done\n",
    "            if len(episode_memory) >= self.seq_len or done:\n",
    "                self.train_on_batch(episode_memory)\n",
    "                episode_memory = deque(maxlen=self.seq_len)\n",
    "\n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "        return episode_reward\n",
    "\n",
    "    def train_on_batch(self, memory):\n",
    "        \"\"\"Train the model on a batch of experiences\"\"\"\n",
    "        if len(memory) == 0:\n",
    "            return\n",
    "\n",
    "        # Use truncated backpropagation through time (tbtt)\n",
    "        states, next_states, hidden_states, actions, rewards, dones = self.tbtt(memory)\n",
    "\n",
    "        # Compute target Q values\n",
    "        with torch.no_grad(), autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "            target_q_values, _ = self.target_model(next_states, hidden_states)\n",
    "            max_next_q_values = torch.max(target_q_values, dim=1, keepdim=True)[0]\n",
    "            target_q = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        # Compute current Q values and loss\n",
    "        self.optimizer.zero_grad()\n",
    "        with autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "            current_q_values, _ = self.model(states, hidden_states)\n",
    "            current_q_values = current_q_values.gather(1, actions)\n",
    "            loss = self.criterion(current_q_values, target_q)\n",
    "\n",
    "        # Backpropagate loss with mixed precision\n",
    "        self.scaler.scale(loss).backward()\n",
    "        self.scaler.step(self.optimizer)\n",
    "        self.scaler.update()\n",
    "\n",
    "    @staticmethod\n",
    "    def tbtt(memory):\n",
    "        \"\"\"Prepare batch for truncated backpropagation through time\"\"\"\n",
    "        if not memory:\n",
    "            return None, None, None, None, None, None\n",
    "\n",
    "        states, actions, rewards, next_states, dones, hidden_states = zip(*memory)\n",
    "        device = next(iter(hidden_states[0][0].parameters())) if hidden_states else torch.device(\"cpu\")\n",
    "\n",
    "        states = torch.tensor(np.array(states), dtype=torch.float32).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.int64).to(device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "        next_states = torch.tensor(np.array(next_states), dtype=torch.float32).to(device)\n",
    "        dones = torch.tensor(dones, dtype=torch.float32).to(device).unsqueeze(1)\n",
    "\n",
    "        # Process hidden states\n",
    "        h_states = (torch.cat([h[0] for h in hidden_states], dim=1),\n",
    "                   torch.cat([h[1] for h in hidden_states], dim=1))\n",
    "\n",
    "        return states, next_states, h_states, actions, rewards, dones\n",
    "\n",
    "    @staticmethod\n",
    "    def expand(state):\n",
    "        \"\"\"Expand state into three components for processing\"\"\"\n",
    "        if isinstance(state, np.ndarray):\n",
    "            return np.expand_dims(state, axis=0), np.expand_dims(state, axis=0), state\n",
    "        else:\n",
    "            # Handle case where state might be a different format\n",
    "            state_array = np.array(state)\n",
    "            return np.expand_dims(state_array, axis=0), np.expand_dims(state_array, axis=0), state_array\n",
    "\n",
    "    def run(self, episodes):\n",
    "        \"\"\"Run the agent for a specified number of episodes\"\"\"\n",
    "        total_rewards = []\n",
    "\n",
    "        for episode in range(episodes):\n",
    "            state = self.envs.reset()\n",
    "            state0, state1, state2 = self.expand(state)\n",
    "            hidden_state = (torch.zeros(self.num_layers, 1, self.hidden_size, dtype=torch.float16).to(self.device),\n",
    "                          torch.zeros(self.num_layers, 1, self.hidden_size, dtype=torch.float16).to(self.device))\n",
    "            episode_reward = 0\n",
    "            episode_memory = deque(maxlen=self.seq_len)\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                state_tensor = torch.tensor(state2, dtype=torch.float32).to(self.device).unsqueeze(0)\n",
    "\n",
    "                if np.random.rand() <= self.epsilon:\n",
    "                    action = self.envs.action_space.sample()\n",
    "                else:\n",
    "                    with torch.no_grad(), autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "                        q_values, hidden_state = self.model(state_tensor, hidden_state)\n",
    "                        action = torch.argmax(q_values, dim=1).item()\n",
    "\n",
    "                next_state, reward, done, _ = self.envs.step(action)\n",
    "                next_state0, next_state1, next_state2 = self.expand(next_state)\n",
    "                episode_reward += reward\n",
    "                episode_memory.append((state2, action, reward, next_state2, done, hidden_state))\n",
    "                state0, state1, state2 = next_state0, next_state1, next_state2\n",
    "\n",
    "                if len(episode_memory) >= self.seq_len or done:\n",
    "                    states, next_states, hidden_states, actions, rewards, dones = self.tbtt(episode_memory)\n",
    "\n",
    "                    if states is not None:\n",
    "                        with torch.no_grad(), autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "                            target_q_values, _ = self.target_model(next_states, hidden_states)\n",
    "                            max_next_q_values = torch.max(target_q_values, dim=1, keepdim=True)[0]\n",
    "                            target_q = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "                        # Get Q values from model\n",
    "                        self.optimizer.zero_grad()\n",
    "                        with autocast(device_type=self.device.type, dtype=torch.float16):\n",
    "                            current_q_values, _ = self.model(states, hidden_states)\n",
    "                            current_q_values = current_q_values.gather(1, actions)\n",
    "                            loss = self.criterion(current_q_values, target_q)\n",
    "\n",
    "                        self.scaler.scale(loss).backward()\n",
    "                        self.scaler.step(self.optimizer)\n",
    "                        self.scaler.update()\n",
    "\n",
    "                    episode_memory = deque(maxlen=self.seq_len)\n",
    "\n",
    "            # Update epsilon\n",
    "            self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "\n",
    "            # Update target network periodically\n",
    "            if episode % 10 == 0:\n",
    "                self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "            total_rewards.append(episode_reward)\n",
    "            print(f\"Episode {episode+1}/{episodes}, Reward: {episode_reward:.2f}, Epsilon: {self.epsilon:.2f}\")\n",
    "\n",
    "        return total_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 15\n",
    "hidden_size = 50\n",
    "output_size = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 30\n",
    "batch_size = 1024\n",
    "memory_size = 100_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Decay Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "num_episodes = 1000\n",
    "gamma = 0.99"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "envs = gym.vector.AsyncVectorEnv([\n",
    "    lambda: make_function(1),\n",
    "    lambda: make_function(2),\n",
    "    lambda: make_function(3)\n",
    "])\n",
    "\n",
    "agent = TrainingAgent(\n",
    "  envs=envs,\n",
    "  hidden_size=hidden_size,\n",
    "  learning_rate=learning_rate,\n",
    "  gamma=gamma,\n",
    "  epsilon=epsilon,\n",
    "  epsilon_min=epsilon_min,\n",
    "  epsilon_decay=epsilon_decay,\n",
    "  seq_len=seq_len\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train(num_episodes=50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
