{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MAX_MEMORY = 500_000  # Increase memory buffer for better long-term learning\n",
    "BATCH_SIZE = 512  # L4 can handle larger batch sizes efficiently\n",
    "LEARNING_RATE = 0.0025  # Reduce LR for more stable convergence\n",
    "GAMMA = 0.99  # Higher discount factor for long-term rewards\n",
    "EPSILON_DECAY = 0.995  # Slower decay to improve exploration\n",
    "MIN_EPSILON = 0.05  # Avoid premature exploitation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True # Optimize CUDA operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "import torch\n",
    "from typing import Tuple, List\n",
    "\n",
    "class Action(Enum):\n",
    "    HOLD = 0\n",
    "    LONG = 1\n",
    "    SHORT = 2\n",
    "    FLATTEN = 3\n",
    "\n",
    "class TradeSide(Enum):\n",
    "    LONG = 0\n",
    "    SHORT = 1\n",
    "\n",
    "class Position:\n",
    "    def __init__(self, entry_price: float, size: int, side: TradeSide):\n",
    "        self.entry_price = entry_price\n",
    "        self.size = size\n",
    "        self.side = side\n",
    "\n",
    "    def get_unrealized_pnl(self, current_price: float) -> float:\n",
    "        diff = (current_price - self.entry_price) if self.side == TradeSide.LONG else (self.entry_price - current_price)\n",
    "        return self.entry_price * self.size * (diff / ((self.entry_price + current_price) / 2) / 100)\n",
    "\n",
    "    def close(self, current_price: float) -> float:\n",
    "        return self.get_unrealized_pnl(current_price)\n",
    "\n",
    "class TradingEnvironment:\n",
    "    REQUIRED_FEATURES = [\n",
    "        \"Asset1_Price\", \"Asset2_Price\", \"Ratio_Price\", \"Spread_ZScore\", \"Rolling_Correlation\",\n",
    "        \"Rolling_Cointegration_Score\", \"RSI1\", \"RSI2\", \"RSI3\",\n",
    "        \"MACD1\", \"MACD2\", \"MACD3\"\n",
    "    ]\n",
    "\n",
    "    PERFORMANCE_COLUMNS = [\"Unrealized_PnL\", \"Realized_PnL\", \"Positioned\"]\n",
    "\n",
    "    def __init__(self, df: torch.Tensor, plot: bool = False, debug: bool = False):\n",
    "        self.step = 0\n",
    "        self.window_size = 1\n",
    "        self.debug = debug\n",
    "        self.plot = plot\n",
    "        self.balance = self.equity = 1_000_000\n",
    "        self.positions: List[Tuple[Position, Position]] = []\n",
    "        self.realized_pnl = self.unrealized_pnl = 0\n",
    "        self.trade_history, self.reward_history = [], []\n",
    "        self.buy_signals, self.sell_signals = [], []\n",
    "        self.losing_streak, self.winning_streak = 0, 0\n",
    "        self.last_trade_step = 0\n",
    "        self.data = df\n",
    "        self._prepare_data()\n",
    "\n",
    "    def _prepare_data(self):\n",
    "      empty_column = torch.zeros(self.data.shape[0], 1).to(device)\n",
    "      for feature in self.PERFORMANCE_COLUMNS:\n",
    "        self.data = torch.cat((self.data, empty_column), dim=1)\n",
    "      print(self.data.shape)\n",
    "\n",
    "    def reset(self) -> torch.Tensor:\n",
    "        self.step = self.realized_pnl = self.unrealized_pnl = 0\n",
    "        self.balance = self.equity = 1_000_000\n",
    "        self.positions.clear()\n",
    "        self.trade_history.clear()\n",
    "        self.reward_history.clear()\n",
    "        self.buy_signals.clear()\n",
    "        self.sell_signals.clear()\n",
    "        self.last_trade_step = 0\n",
    "        return self.current_observation()\n",
    "\n",
    "    def step_forward(self, action: Action) -> Tuple[torch.Tensor, float, float, float, bool]:\n",
    "        self.step += 1\n",
    "\n",
    "        reward, profit, positioned = self.execute_trade(action)\n",
    "        self.trade_history.append(self.realized_pnl)\n",
    "        self.reward_history.append(reward)\n",
    "\n",
    "        if self.data is None:\n",
    "            if self.debug:\n",
    "                print(\"DEBUG: self.data is None in step_forward\")\n",
    "            done = True\n",
    "        else:\n",
    "            done = self.step + self.window_size >= len(self.data)\n",
    "\n",
    "        obs = self.current_observation()\n",
    "        obs[13] = self.unrealized_pnl\n",
    "        obs[14] = self.realized_pnl\n",
    "        obs[15] = positioned\n",
    "        return obs, reward, self.realized_pnl, self.unrealized_pnl, done\n",
    "\n",
    "    def execute_trade(self, action: Action) -> Tuple[float, float, int]:\n",
    "        asset1_price = self.data[self.step, 0].item()\n",
    "        asset2_price = self.data[self.step, 1].item()\n",
    "        hedge_ratio = self.data[self.step, 3].item()\n",
    "\n",
    "        reward = 0\n",
    "        size = 100_000\n",
    "        profit = 0\n",
    "\n",
    "        if action == Action.HOLD:\n",
    "            self.update_unrealized_pnl()\n",
    "            self.last_trade_step += 1\n",
    "\n",
    "        elif action in (Action.LONG, Action.SHORT):\n",
    "            positions = (\n",
    "                Position(asset1_price, size, TradeSide.LONG if action == Action.LONG else TradeSide.SHORT),\n",
    "                Position(asset2_price, int(size * hedge_ratio), TradeSide.SHORT if action == Action.LONG else TradeSide.LONG)\n",
    "            )\n",
    "            self.positions.append(positions)\n",
    "            (self.buy_signals if action == Action.LONG else self.sell_signals).append(self.step)\n",
    "            self.last_trade_step = 0\n",
    "            if len(positions) > 2:\n",
    "                reward = self.calculate_reward(self.realized_pnl, self.unrealized_pnl, True, True)\n",
    "            elif len(positions) == 2:\n",
    "                reward = self.calculate_reward(self.realized_pnl, self.unrealized_pnl, True, False)\n",
    "\n",
    "        elif action == Action.FLATTEN and self.positions:\n",
    "            pnl = 0\n",
    "            for positions in self.positions:\n",
    "                pnl += positions[0].close(asset1_price) + positions[1].close(asset2_price)\n",
    "                profit += pnl\n",
    "                self.realized_pnl += pnl\n",
    "            self.unrealized_pnl = 0\n",
    "            self.positions.clear()\n",
    "            reward = self.calculate_reward(pnl, self.unrealized_pnl, False, False)\n",
    "\n",
    "        return reward, profit, int(bool(self.positions))\n",
    "\n",
    "    def update_unrealized_pnl(self):\n",
    "      if not self.positions:\n",
    "          self.unrealized_pnl = 0\n",
    "          return\n",
    "\n",
    "      self.unrealized_pnl = sum(\n",
    "          pos_a.get_unrealized_pnl(self.data[self.step, 0].item()) + pos_b.get_unrealized_pnl(self.data[self.step, 1].item())\n",
    "          for pos_a, pos_b in self.positions\n",
    "      )\n",
    "\n",
    "    def calculate_reward(\n",
    "        self,\n",
    "        rpnl: float,\n",
    "        upnl: float,\n",
    "        entry: bool,\n",
    "        dca: bool = False,\n",
    "        transaction_costs: float = 0.001,\n",
    "        risk_free_rate: float = 0.02,\n",
    "        decay_factor: float = 0.99\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Enhanced reward function with smooth normalization for statistical arbitrage.\n",
    "\n",
    "        Parameters:\n",
    "        - rpnl: Realized Profit and Loss\n",
    "        - upnl: Unrealized Profit and Loss\n",
    "        - entry: Whether this is a trade entry point\n",
    "        - transaction_costs: Cost per trade\n",
    "        - risk_free_rate: Benchmark risk-free rate for comparison\n",
    "        - decay_factor: Decay factor for historical rewards to prevent reward inflation\n",
    "\n",
    "        Returns:\n",
    "        - Normalized reward score\n",
    "        \"\"\"\n",
    "        if not hasattr(self, '_reward_history'):\n",
    "            self._reward_history = []\n",
    "\n",
    "        reward = 0.0\n",
    "        net_profit = rpnl - transaction_costs\n",
    "        profit_reward = np.sign(net_profit) * np.log1p(abs(net_profit))\n",
    "        reward += profit_reward\n",
    "\n",
    "        if entry:\n",
    "            if dca:\n",
    "              reward -= 1\n",
    "            lookahead_steps = min(30, len(self.data) - self.step)\n",
    "            lookforward1 = self.data[self.step:self.step + lookahead_steps, 0]\n",
    "            lookforward2 = self.data[self.step:self.step + lookahead_steps, 1]\n",
    "\n",
    "            entry_mfe1 = max(lookforward1.max() - self.data[self.step, 0], 0)\n",
    "            entry_mae1 = min(lookforward1.min() - self.data[self.step, 0], 0)\n",
    "            entry_mfe2 = max(lookforward2.max() - self.data[self.step, 1], 0)\n",
    "            entry_mae2 = min(lookforward2.min() - self.data[self.step, 1], 0)\n",
    "\n",
    "            entry_mfe1_cpu = entry_mfe1.cpu().detach().numpy()\n",
    "            entry_mae1_cpu = entry_mae1.cpu().detach().numpy()\n",
    "\n",
    "            entry_mfe2_cpu = entry_mfe2.cpu().detach().numpy()\n",
    "            entry_mae2_cpu = entry_mae2.cpu().detach().numpy()\n",
    "\n",
    "            entry_quality1 = np.arctan(entry_mfe1_cpu - abs(entry_mae1_cpu))\n",
    "            entry_quality2 = np.arctan(entry_mfe2_cpu - abs(entry_mae2_cpu))\n",
    "            reward += 0.5 * (entry_quality1 + entry_quality2)\n",
    "        elif not entry:\n",
    "            streak_component = 0\n",
    "            if rpnl > 10:\n",
    "                self.winning_streak += 1\n",
    "                self.losing_streak = 0\n",
    "            else:\n",
    "                self.losing_streak += 1\n",
    "                self.winning_streak = 0\n",
    "\n",
    "            if self.losing_streak > 3:\n",
    "                streak_component = self.losing_streak * -0.6\n",
    "            elif self.winning_streak > 3:\n",
    "                streak_component = self.winning_streak * 0.7\n",
    "            reward += streak_component\n",
    "\n",
    "        self.update_unrealized_pnl()\n",
    "        if self.unrealized_pnl < 100:\n",
    "            reward -= 1\n",
    "        elif self.unrealized_pnl > 200:\n",
    "            reward += 1\n",
    "\n",
    "        self._reward_history.append(reward)\n",
    "        final_reward = reward\n",
    "        return final_reward\n",
    "\n",
    "    def current_observation(self) -> torch.Tensor:\n",
    "      # Check if self.data is None\n",
    "      if self.data is None:\n",
    "          print(\"DEBUG: self.data is None in current_observation\")\n",
    "          # Return a tensor of zeros with the shape of REQUIRED_FEATURES + PERFORMANCE_COLUMNS\n",
    "          return torch.zeros(len(self.REQUIRED_FEATURES) + len(self.PERFORMANCE_COLUMNS))\n",
    "\n",
    "      # Check if step is out of bounds\n",
    "      elif self.step >= len(self.data):\n",
    "          print(f\"DEBUG: Step index {self.step} is out of bounds for data length {len(self.data)}\")\n",
    "          # Return the last observation in self.data\n",
    "          return self.data[-1].clone()  # Use .clone() to return a copy, similar to pandas `.copy()`\n",
    "\n",
    "      else:\n",
    "          # Return the observation at the current step\n",
    "          return self.data[self.step].clone()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class LSTM_Q_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.3, bidirectional=True):\n",
    "        super(LSTM_Q_Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Bidirectional LSTM (if applicable)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,  # Multiple layers of LSTM\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size * (2 if bidirectional else 1), 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If the input is a 1D tensor (e.g., single time step), reshape it\n",
    "        if len(x.shape) == 1:  # (input_size,)\n",
    "            x = x.unsqueeze(0).unsqueeze(0)  # Add batch and sequence dimensions -> (1, 1, input_size)\n",
    "\n",
    "        # LSTM expects input shape: (batch_size, sequence_length, input_size)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_step = lstm_out  # Get the output of the last time step\n",
    "\n",
    "        # Pass through fully connected layers\n",
    "        x = F.sigmoid(self.fc1(last_step))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Proper weight initialization for LSTM and fully connected layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    # Xavier initialization for LSTM weights\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    # Xavier initialization for fully connected layer weights\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                # Initialize biases to zero\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        model_folder_path = \"./models\"\n",
    "        os.makedirs(model_folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma, batch_size, target_update_freq=50):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model.to(device)\n",
    "\n",
    "        # Create target network with same architecture\n",
    "        self.target_model = LSTM_Q_Net(\n",
    "            input_size=16,\n",
    "            hidden_size=model.hidden_size,\n",
    "            output_size=model.fc2.out_features,\n",
    "            bidirectional=model.bidirectional\n",
    "        ).to(device)\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "        # Use Adam with improved parameters\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr, amsgrad=True)\n",
    "\n",
    "        # Less frequent LR adjustments\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "        # Huber loss (SmoothL1Loss) is more robust for RL\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_step_count = 0\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Reshape action for gathering\n",
    "        action = action.view(-1, 1)\n",
    "\n",
    "        # Get current Q values\n",
    "        pred = self.model(state)\n",
    "\n",
    "        # Implement Double Q-learning for stability\n",
    "        with torch.no_grad():\n",
    "            # Get actions from main network\n",
    "            next_actions = self.model(next_state).argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # Get Q-values from target network\n",
    "            next_q_values = self.target_model(next_state).gather(1, next_actions)\n",
    "\n",
    "            # Compute target Q values\n",
    "            target = reward.unsqueeze(1) + (1 - done.float().unsqueeze(1)) * self.gamma * next_q_values\n",
    "\n",
    "        # Get Q values for taken actions\n",
    "        q_values = pred.gather(1, action)\n",
    "\n",
    "        # Calculate loss and optimize\n",
    "        loss = self.criterion(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network less frequently\n",
    "        self.train_step_count += 1\n",
    "        if self.train_step_count % self.target_update_freq == 0:\n",
    "            self.update_target()\n",
    "            self.scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "class TradingAgent:\n",
    "    def __init__(self, data: torch.Tensor, plot: bool = False, debug: bool = False):\n",
    "        self.debug = debug\n",
    "        self.epsilon = 0.25\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.data = data\n",
    "        self.model = LSTM_Q_Net(input_size=16, hidden_size=128, output_size=4).to(device)\n",
    "        self.trainer = QTrainer(self.model, lr=LEARNING_RATE, gamma=GAMMA, batch_size=BATCH_SIZE)\n",
    "        self.env = TradingEnvironment(data, plot, debug)\n",
    "        self.equity_curves = {}\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Stores experiences with clipped rewards.\"\"\"\n",
    "        if reward is None or np.isnan(reward):\n",
    "            if self.debug:\n",
    "                print(\"DEBUG: Received None or NaN reward, setting to 0.0\")\n",
    "            reward = 0.0\n",
    "        reward = np.clip(reward, -1, 1)\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_experiences(self):\n",
    "        \"\"\"Priority experience replay with absolute reward-based weighting.\"\"\"\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return list(self.memory)\n",
    "\n",
    "        rewards = np.array([exp[2] for exp in self.memory], dtype=np.float32)\n",
    "        priorities = np.abs(rewards) + 1e-6  # Avoid zero probabilities\n",
    "        probabilities = priorities / np.sum(priorities)\n",
    "\n",
    "        indices = np.random.choice(len(self.memory), BATCH_SIZE, p=probabilities, replace=False)\n",
    "        return [self.memory[i] for i in indices]\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        \"\"\"Decay epsilon for exploration-exploitation balance.\"\"\"\n",
    "        self.epsilon = max(MIN_EPSILON, self.epsilon * EPSILON_DECAY)\n",
    "\n",
    "    def select_action(self, state: torch.Tensor) -> Tuple[Action, int]:\n",
    "        \"\"\"Selects an action using epsilon-greedy strategy.\"\"\"\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action_idx = np.random.randint(0, 4)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                action_idx = self.model(state.unsqueeze(0)).argmax().item()\n",
    "\n",
    "        return [Action.HOLD, Action.LONG, Action.SHORT, Action.FLATTEN][action_idx], action_idx\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Trains the model using batched experience replay.\"\"\"\n",
    "        batch = self.sample_experiences()\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        # Convert to tensors\n",
    "        states = torch.stack(states).to(device)\n",
    "        next_states = torch.stack(next_states).to(device)\n",
    "        actions = torch.tensor(actions, dtype=torch.long, device=device).unsqueeze(1)\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32, device=device).unsqueeze(1)\n",
    "        dones = torch.tensor(dones, dtype=torch.bool, device=device).unsqueeze(1)\n",
    "\n",
    "        return self.trainer.train_step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def save_model(self, episode: int):\n",
    "        \"\"\"Saves the trained model with error handling.\"\"\"\n",
    "        try:\n",
    "            filename = f\"Episode-{episode}.pth\" if episode is not None else \"Episode-unknown.pth\"\n",
    "            self.model.save(filename)\n",
    "            if self.debug:\n",
    "                print(f\"DEBUG: Model saved as {filename}\")\n",
    "        except Exception as e:\n",
    "            print(f\"DEBUG: Model save failed - {str(e)}\")\n",
    "\n",
    "    def run(self, episodes: int):\n",
    "        \"\"\"Runs training episodes with performance tracking.\"\"\"\n",
    "        for episode in tqdm(range(episodes), desc=\"Training Progress\", position=0):\n",
    "            state, done = self.env.reset(), False\n",
    "            equity_curve, episode_rewards, episode_losses = [], [], []\n",
    "            total_steps, max_drawdown, peak_equity = 0, 0, 0\n",
    "\n",
    "            with tqdm(total=len(self.data), desc=f\"Episode {episode + 1}\", leave=False, position=1) as step_bar:\n",
    "                while not done:\n",
    "                    action, action_idx = self.select_action(state)\n",
    "                    next_state, reward, real_profit, _, done = self.env.step_forward(action)\n",
    "\n",
    "                    equity_curve.append(real_profit)\n",
    "                    episode_rewards.append(reward)\n",
    "                    total_steps += 1\n",
    "\n",
    "                    peak_equity = max(peak_equity, real_profit)\n",
    "                    max_drawdown = max(max_drawdown, (peak_equity - real_profit) / (peak_equity + 1e-8))\n",
    "\n",
    "                    self.store_experience(state, action_idx, reward, next_state, done)\n",
    "                    loss = self.train()\n",
    "                    if loss is not None:\n",
    "                        episode_losses.append(loss.item())\n",
    "\n",
    "                    state = next_state\n",
    "                    step_bar.update(1)\n",
    "\n",
    "            self.equity_curves[episode] = equity_curve\n",
    "\n",
    "            # Episode summary\n",
    "            print({\n",
    "                'episode': episode,\n",
    "                'total_steps': total_steps,\n",
    "                'total_reward': np.sum(episode_rewards),\n",
    "                'mean_reward': np.mean(episode_rewards) if episode_rewards else 0,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'last_loss': episode_losses[-1] if episode_losses else None\n",
    "            })\n",
    "\n",
    "            self.update_epsilon()\n",
    "            self.save_model(episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of data: [('2016-01-04 00:29:00', 0.43808314, 0.93958347, 0.02263698, 0.41702082, 0.33888563, 0.91981418, 1., 0.26679839, 0.38031375, 0.35007507, 0.51645163, 0.73697773, 0.29400669)\n",
      " ('2016-01-04 00:30:00', 0.43791593, 0.93928257, 0.02280172, 0.41696028, 0.40237618, 0.93247409, 1., 0.25321426, 0.32934684, 0.38543971, 0.51526629, 0.7364399 , 0.29387281)\n",
      " ('2016-01-04 00:31:00', 0.43808314, 0.93939003, 0.02284263, 0.41721442, 0.4067852 , 0.93922893, 1., 0.29190755, 0.36218071, 0.39425633, 0.51481955, 0.73618861, 0.29387499)\n",
      " ('2016-01-04 00:32:00', 0.43851787, 0.93923958, 0.02340607, 0.41651643, 0.53328086, 0.93713808, 1., 0.38164029, 0.33724664, 0.50055925, 0.51547785, 0.73584345, 0.29464702)\n",
      " ('2016-01-04 00:33:00', 0.43922015, 0.93930406, 0.02398924, 0.41545451, 0.63931675, 0.92454615, 1., 0.49338477, 0.3576291 , 0.58232302, 0.51751565, 0.73569638, 0.29603847)]\n",
      "Column names: ('Gmt_time', 'Asset1_Price', 'Asset2_Price', 'Ratio_Price', 'Hedge_Ratio', 'Spread_ZScore', 'Rolling_Correlation', 'Rolling_Cointegration_Score', 'RSI1', 'RSI2', 'RSI3', 'MACD1', 'MACD2', 'MACD3')\n",
      "Shape of the tensor: torch.Size([1993777, 13])\n",
      "torch.Size([1993777, 16])\n"
     ]
    }
   ],
   "source": [
    "data = np.genfromtxt('data/processed/train_data_scaled.csv', delimiter=',', skip_header=False, dtype=None, names=True, encoding=None)\n",
    "print(f\"First few rows of data: {data[:5]}\")\n",
    "print(f\"Column names: {data.dtype.names}\")\n",
    "data_removed = np.column_stack([data[field] for field in data.dtype.names[1:]])\n",
    "data_tensor = torch.tensor(data_removed, dtype=torch.float32).to(device)\n",
    "print(f\"Shape of the tensor: {data_tensor.shape}\")\n",
    "\n",
    "agent = TradingAgent(data_tensor, plot=False, debug=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 80\u001b[39m, in \u001b[36mTradingAgent.run\u001b[39m\u001b[34m(self, episodes)\u001b[39m\n\u001b[32m     78\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, episodes: \u001b[38;5;28mint\u001b[39m):\n\u001b[32m     79\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Runs training episodes with performance tracking.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepisodes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining Progress\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m     81\u001b[39m         state, done = \u001b[38;5;28mself\u001b[39m.env.reset(), \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m     82\u001b[39m         equity_curve, episode_rewards, episode_losses = [], [], []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\notebook.py:234\u001b[39m, in \u001b[36mtqdm_notebook.__init__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    232\u001b[39m unit_scale = \u001b[32m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.unit_scale \u001b[38;5;129;01mor\u001b[39;00m \u001b[32m1\u001b[39m\n\u001b[32m    233\u001b[39m total = \u001b[38;5;28mself\u001b[39m.total * unit_scale \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.total\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m \u001b[38;5;28mself\u001b[39m.container = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstatus_printer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mncols\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[38;5;28mself\u001b[39m.container.pbar = proxy(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m    236\u001b[39m \u001b[38;5;28mself\u001b[39m.displayed = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\site-packages\\tqdm\\notebook.py:108\u001b[39m, in \u001b[36mtqdm_notebook.status_printer\u001b[39m\u001b[34m(_, total, desc, ncols)\u001b[39m\n\u001b[32m     99\u001b[39m \u001b[38;5;66;03m# Fallback to text bar if there's no total\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[38;5;66;03m# DEPRECATED: replaced with an 'info' style bar\u001b[39;00m\n\u001b[32m    101\u001b[39m \u001b[38;5;66;03m# if not total:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    105\u001b[39m \n\u001b[32m    106\u001b[39m \u001b[38;5;66;03m# Prepare IPython progress bar\u001b[39;00m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m IProgress \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:  \u001b[38;5;66;03m# #187 #451 #558 #872\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m108\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(WARN_NOIPYW)\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m total:\n\u001b[32m    110\u001b[39m     pbar = IProgress(\u001b[38;5;28mmin\u001b[39m=\u001b[32m0\u001b[39m, \u001b[38;5;28mmax\u001b[39m=total)\n",
      "\u001b[31mImportError\u001b[39m: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "agent.run(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
