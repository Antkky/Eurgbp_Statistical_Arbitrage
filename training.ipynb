{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "MAX_MEMORY = 500_000  # Increase memory buffer for better long-term learning\n",
    "BATCH_SIZE = 512  # L4 can handle larger batch sizes efficiently\n",
    "LEARNING_RATE = 0.0025  # Reduce LR for more stable convergence\n",
    "GAMMA = 0.99  # Higher discount factor for long-term rewards\n",
    "EPSILON_DECAY = 0.995  # Slower decay to improve exploration\n",
    "MIN_EPSILON = 0.05  # Avoid premature exploitation\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True # Optimize CUDA operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from typing import Tuple, List\n",
    "\n",
    "class Action(Enum):\n",
    "    HOLD = 0\n",
    "    LONG = 1\n",
    "    SHORT = 2\n",
    "    FLATTEN = 3\n",
    "\n",
    "class TradeSide(Enum):\n",
    "    LONG = 0\n",
    "    SHORT = 1\n",
    "\n",
    "class Position:\n",
    "    def __init__(self, entry_price: float, size: int, side: TradeSide):\n",
    "        self.entry_price = entry_price\n",
    "        self.size = size\n",
    "        self.side = side\n",
    "\n",
    "    def get_unrealized_pnl(self, current_price: float) -> float:\n",
    "        diff = (current_price - self.entry_price) if self.side == TradeSide.LONG else (self.entry_price - current_price)\n",
    "        return self.entry_price * self.size * (diff / ((self.entry_price + current_price) / 2) / 100)\n",
    "\n",
    "    def close(self, current_price: float) -> float:\n",
    "        return self.get_unrealized_pnl(current_price)\n",
    "\n",
    "class TradingEnvironment:\n",
    "    REQUIRED_FEATURES = [\n",
    "        \"Asset1_Price\", \"Asset2_Price\", \"Ratio_Price\", \"Spread_ZScore\", \"Rolling_Correlation\",\n",
    "        \"Rolling_Cointegration_Score\", \"RSI1\", \"RSI2\", \"RSI3\",\n",
    "        \"MACD1\", \"MACD2\", \"MACD3\"\n",
    "    ]\n",
    "\n",
    "    PERFORMANCE_COLUMNS = [\"Unrealized_PnL\", \"Realized_PnL\", \"Positioned\"]\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, plot: bool = False, debug: bool = False):\n",
    "        self.step = 0\n",
    "        self.window_size = 1\n",
    "        self.debug = debug\n",
    "        self.plot = plot\n",
    "        self.balance = self.equity = 1_000_000\n",
    "        self.positions: List[Tuple[Position, Position]] = []\n",
    "        self.realized_pnl = self.unrealized_pnl = 0\n",
    "        self.trade_history, self.reward_history = [], []\n",
    "        self.buy_signals, self.sell_signals = [], []\n",
    "        self.losing_streak, self.winning_streak = 0, 0\n",
    "        self.last_trade_step = 0\n",
    "        self._prepare_data(df)\n",
    "\n",
    "    def _prepare_data(self, df: pd.DataFrame):\n",
    "        missing_features = [f for f in self.REQUIRED_FEATURES if f not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "        self.data = df.copy()[self.REQUIRED_FEATURES]\n",
    "        for col in self.PERFORMANCE_COLUMNS:\n",
    "            self.data[col] = 0\n",
    "\n",
    "        if \"Hedge_Ratio\" not in df.columns:\n",
    "            raise ValueError(\"Missing 'Hedge_Ratio' column\")\n",
    "\n",
    "        self.hedge_ratio = df[\"Hedge_Ratio\"]\n",
    "\n",
    "    def reset(self) -> pd.Series:\n",
    "        self.step = self.realized_pnl = self.unrealized_pnl = 0\n",
    "        self.balance = self.equity = 1_000_000\n",
    "        self.positions.clear()\n",
    "        self.trade_history.clear()\n",
    "        self.reward_history.clear()\n",
    "        self.buy_signals.clear()\n",
    "        self.sell_signals.clear()\n",
    "        self.last_trade_step = 0\n",
    "        return self.current_observation()\n",
    "\n",
    "    def step_forward(self, action: Action) -> Tuple[pd.Series, float, float, float, bool]:\n",
    "        self.step += 1\n",
    "        reward, profit, positioned = self.execute_trade(action)\n",
    "\n",
    "        self.trade_history.append(self.realized_pnl)\n",
    "        self.reward_history.append(reward)\n",
    "\n",
    "        # Fixed: Added debugging and proper None check\n",
    "        if self.data is None:\n",
    "            if self.debug:\n",
    "              print(\"DEBUG: self.data is None in step_forward\")\n",
    "            done = True\n",
    "        else:\n",
    "            done = self.step + self.window_size >= len(self.data)\n",
    "\n",
    "        # Get a copy of the observation to avoid SettingWithCopyWarning\n",
    "        obs = self.current_observation()\n",
    "\n",
    "        # Update the copy with performance metrics\n",
    "        obs.update({\"Unrealized_PnL\": self.unrealized_pnl, \"Realized_PnL\": self.realized_pnl, \"Positioned\": positioned})\n",
    "\n",
    "        return obs, reward, self.realized_pnl, self.unrealized_pnl, done\n",
    "\n",
    "    def execute_trade(self, action: Action) -> Tuple[float, float, int]:\n",
    "      asset1_price = self.data.iloc[self.step][\"Asset1_Price\"]\n",
    "      asset2_price = self.data.iloc[self.step][\"Asset2_Price\"]\n",
    "      hedge_ratio = self.hedge_ratio.iloc[self.step]\n",
    "\n",
    "      profit = reward = 0\n",
    "      size = 100_000\n",
    "\n",
    "      if action == Action.HOLD:\n",
    "          self.update_unrealized_pnl()\n",
    "          self.last_trade_step += 1\n",
    "\n",
    "      elif action in (Action.LONG, Action.SHORT):\n",
    "          positions = (\n",
    "              Position(asset1_price, size, TradeSide.LONG if action == Action.LONG else TradeSide.SHORT),\n",
    "              Position(asset2_price, int(size * hedge_ratio), TradeSide.SHORT if action == Action.LONG else TradeSide.LONG)\n",
    "          )\n",
    "          self.positions.append(positions)\n",
    "          (self.buy_signals if action == Action.LONG else self.sell_signals).append(self.step)\n",
    "          self.last_trade_step = 0\n",
    "          if len(positions) > 2:\n",
    "              reward = self.calculate_reward(self.realized_pnl, self.unrealized_pnl, True, True)\n",
    "          elif len(positions) == 2:\n",
    "              reward = self.calculate_reward(self.realized_pnl, self.unrealized_pnl, True, False)\n",
    "\n",
    "      elif action == Action.FLATTEN and self.positions:\n",
    "          pnl = sum(pos_a.close(asset1_price) + pos_b.close(asset2_price) for pos_a, pos_b in self.positions)\n",
    "          self.realized_pnl += pnl\n",
    "          self.unrealized_pnl = 0\n",
    "          self.positions.clear()\n",
    "          profit = pnl\n",
    "          reward = self.calculate_reward(pnl, self.unrealized_pnl, False, False)\n",
    "      return reward, profit, int(bool(self.positions))\n",
    "\n",
    "    def update_unrealized_pnl(self):\n",
    "      if not self.positions:\n",
    "          self.unrealized_pnl = 0\n",
    "          return\n",
    "\n",
    "      self.unrealized_pnl = sum(\n",
    "          pos_a.get_unrealized_pnl(self.data['Asset1_Price'].iloc[self.step]) + pos_b.get_unrealized_pnl(self.data['Asset2_Price'].iloc[self.step])\n",
    "          for pos_a, pos_b in self.positions\n",
    "      )\n",
    "\n",
    "    def calculate_reward(\n",
    "        self,\n",
    "        rpnl: float,\n",
    "        upnl: float,\n",
    "        entry: bool,\n",
    "        dca: bool = False,\n",
    "        transaction_costs: float = 0.001,\n",
    "        risk_free_rate: float = 0.02,\n",
    "        decay_factor: float = 0.99\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Enhanced reward function with smooth normalization for statistical arbitrage.\n",
    "\n",
    "        Parameters:\n",
    "        - rpnl: Realized Profit and Loss\n",
    "        - upnl: Unrealized Profit and Loss\n",
    "        - entry: Whether this is a trade entry point\n",
    "        - transaction_costs: Cost per trade\n",
    "        - risk_free_rate: Benchmark risk-free rate for comparison\n",
    "        - decay_factor: Decay factor for historical rewards to prevent reward inflation\n",
    "\n",
    "        Returns:\n",
    "        - Normalized reward score\n",
    "        \"\"\"\n",
    "        if not hasattr(self, '_reward_history'):\n",
    "            self._reward_history = []\n",
    "\n",
    "        reward = 0.0\n",
    "        net_profit = rpnl - transaction_costs\n",
    "        profit_reward = np.sign(net_profit) * np.log1p(abs(net_profit))\n",
    "        reward += profit_reward\n",
    "\n",
    "        if entry:\n",
    "            if dca:\n",
    "              reward -= 1\n",
    "            lookahead_steps = min(30, len(self.data) - self.step)\n",
    "            lookforward1 = self.data['Asset1_Price'].iloc[self.step: self.step + lookahead_steps]\n",
    "            lookforward2 = self.data['Asset2_Price'].iloc[self.step: self.step + lookahead_steps]\n",
    "\n",
    "            entry_mfe1 = max(lookforward1.max() - self.data['Asset1_Price'].iloc[self.step], 0)\n",
    "            entry_mae1 = min(lookforward1.min() - self.data['Asset1_Price'].iloc[self.step], 0)\n",
    "            entry_mfe2 = max(lookforward2.max() - self.data['Asset2_Price'].iloc[self.step], 0)\n",
    "            entry_mae2 = min(lookforward2.min() - self.data['Asset2_Price'].iloc[self.step], 0)\n",
    "\n",
    "            entry_quality1 = np.arctan(entry_mfe1 - abs(entry_mae1))\n",
    "            entry_quality2 = np.arctan(entry_mfe2 - abs(entry_mae2))\n",
    "            reward += 0.5 * (entry_quality1 + entry_quality2)\n",
    "        elif not entry:\n",
    "            streak_component = 0\n",
    "            if rpnl > 10:\n",
    "                self.winning_streak += 1\n",
    "                self.losing_streak = 0\n",
    "            else:\n",
    "                self.losing_streak += 1\n",
    "                self.winning_streak = 0\n",
    "\n",
    "            if self.losing_streak > 3:\n",
    "                streak_component = self.losing_streak * -0.6\n",
    "            elif self.winning_streak > 3:\n",
    "                streak_component = self.winning_streak * 0.7\n",
    "            reward += streak_component\n",
    "\n",
    "        self.update_unrealized_pnl()\n",
    "        if self.unrealized_pnl < 100:\n",
    "            reward -= 1\n",
    "        elif self.unrealized_pnl > 200:\n",
    "            reward += 1\n",
    "\n",
    "        self._reward_history.append(reward)\n",
    "        final_reward = reward\n",
    "        return final_reward\n",
    "\n",
    "    def current_observation(self) -> pd.Series:\n",
    "        if self.data is None:\n",
    "            print(\"DEBUG: self.data is None in current_observation\")\n",
    "            return pd.Series({feature: 0 for feature in self.REQUIRED_FEATURES + self.PERFORMANCE_COLUMNS})\n",
    "        elif self.step >= len(self.data):\n",
    "            print(f\"DEBUG: Step index {self.step} is out of bounds for data length {len(self.data)}\")\n",
    "            return self.data.iloc[-1].copy()\n",
    "        else:\n",
    "            return self.data.iloc[self.step].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "\n",
    "class LSTM_Q_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.3, bidirectional=True):\n",
    "        super(LSTM_Q_Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Bidirectional LSTM (if applicable)\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2,  # Multiple layers of LSTM\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_size * (2 if bidirectional else 1), 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        x = F.sigmoid(self.fc1(last_step))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Proper weight initialization for LSTM and fully connected layers\n",
    "        for name, param in self.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                if 'lstm' in name:\n",
    "                    # Xavier initialization for LSTM weights\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "                else:\n",
    "                    # Xavier initialization for fully connected layer weights\n",
    "                    nn.init.xavier_uniform_(param)\n",
    "            elif 'bias' in name:\n",
    "                # Initialize biases to zero\n",
    "                nn.init.zeros_(param)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        model_folder_path = \"./models\"\n",
    "        os.makedirs(model_folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma, batch_size, target_update_freq=50):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model.to(device)\n",
    "\n",
    "        # Create target network with same architecture\n",
    "        self.target_model = LSTM_Q_Net(\n",
    "            input_size=15,\n",
    "            hidden_size=model.hidden_size,\n",
    "            output_size=model.fc2.out_features,\n",
    "            bidirectional=model.bidirectional\n",
    "        ).to(device)\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "        # Use Adam with improved parameters\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr, amsgrad=True)\n",
    "\n",
    "        # Less frequent LR adjustments\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "        # Huber loss (SmoothL1Loss) is more robust for RL\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_step_count = 0\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Reshape action for gathering\n",
    "        action = action.view(-1, 1)\n",
    "\n",
    "        # Get current Q values\n",
    "        pred = self.model(state)\n",
    "\n",
    "        # Implement Double Q-learning for stability\n",
    "        with torch.no_grad():\n",
    "            # Get actions from main network\n",
    "            next_actions = self.model(next_state).argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # Get Q-values from target network\n",
    "            next_q_values = self.target_model(next_state).gather(1, next_actions)\n",
    "\n",
    "            # Compute target Q values\n",
    "            target = reward.unsqueeze(1) + (1 - done.float().unsqueeze(1)) * self.gamma * next_q_values\n",
    "\n",
    "        # Get Q values for taken actions\n",
    "        q_values = pred.gather(1, action)\n",
    "\n",
    "        # Calculate loss and optimize\n",
    "        loss = self.criterion(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network less frequently\n",
    "        self.train_step_count += 1\n",
    "        if self.train_step_count % self.target_update_freq == 0:\n",
    "            self.update_target()\n",
    "            self.scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import optim\n",
    "from collections import deque\n",
    "from tqdm.notebook import tqdm\n",
    "from typing import Tuple\n",
    "\n",
    "class TradingAgent:\n",
    "    def __init__(self, data: pd.DataFrame, plot: bool = False, debug: bool = False):\n",
    "        self.debug = debug\n",
    "        self.epsilon = 0.25\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.data = data\n",
    "        self.model = LSTM_Q_Net(input_size=15, hidden_size=128, output_size=4).to(device)\n",
    "        self.trainer = QTrainer(self.model, lr=LEARNING_RATE, gamma=GAMMA, batch_size=BATCH_SIZE)\n",
    "        self.env = TradingEnvironment(data, plot, debug)\n",
    "        self.equity_curves = {}\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "      if reward is None or np.isnan(reward):\n",
    "          print(\"DEBUG: Received None or NaN reward, setting to 0.0\")\n",
    "          reward = 0.0\n",
    "      else:\n",
    "          reward = np.clip(reward, -1, 1)\n",
    "\n",
    "      self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def sample_experiences(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return list(self.memory)  # Return whatever is available\n",
    "\n",
    "        priorities = np.abs(np.array([exp[2] for exp in self.memory]))  # Absolute rewards\n",
    "        sum_priorities = np.sum(priorities)\n",
    "\n",
    "        # Ensure priorities are non-zero by adding a small constant\n",
    "        if sum_priorities == 0:\n",
    "            probabilities = np.ones(len(self.memory)) / len(self.memory)  # Uniform sampling\n",
    "        else:\n",
    "            probabilities = (priorities + 1e-6) / (sum_priorities + 1e-6)  # Avoid division by zero\n",
    "\n",
    "        # FIX: Normalize probabilities to ensure they sum to exactly 1\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "\n",
    "        sample_size = min(BATCH_SIZE, len(self.memory))\n",
    "        indices = np.random.choice(len(self.memory), sample_size, p=probabilities, replace=False)\n",
    "\n",
    "        return [self.memory[i] for i in indices]\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(MIN_EPSILON, self.epsilon * EPSILON_DECAY)\n",
    "\n",
    "    def select_action(self, state: pd.Series) -> Tuple[Action, int]:\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action_idx = np.random.randint(0, 4)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "              state_tensor = torch.as_tensor(np.nan_to_num(state.values), dtype=torch.float32, device=device).unsqueeze(0)\n",
    "              action_idx = torch.argmax(self.model(state_tensor)).item()\n",
    "\n",
    "        return [Action.HOLD, Action.LONG, Action.SHORT, Action.FLATTEN][action_idx], action_idx\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model using a mini-batch from experience replay.\n",
    "        \"\"\"\n",
    "        batch = self.sample_experiences()\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        self.trainer.train_step(\n",
    "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.long).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "        )\n",
    "\n",
    "    def save_model(self, episode: int):\n",
    "      \"\"\"\n",
    "      Saves the trained model with proper error handling.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          if episode is not None:\n",
    "              print(f\"DEBUG: Saving model for episode {episode}\")\n",
    "              self.model.save(f\"Episode-{episode}.pth\")\n",
    "          else:\n",
    "              print(\"DEBUG: Episode variable is None, using fallback name\")\n",
    "              self.model.save(\"Episode-unknown.pth\")\n",
    "      except Exception as e:\n",
    "          print(f\"DEBUG: Error in save_model - {str(e)}\")\n",
    "\n",
    "    def run(self, episodes: int):\n",
    "        \"\"\"\n",
    "        Execute training episodes with Jupyter-optimized progress tracking.\n",
    "\n",
    "        Args:\n",
    "            episodes (int): Number of training episodes to run\n",
    "        \"\"\"\n",
    "        for episode in tqdm(range(episodes), desc=\"Training Progress\", position=0):\n",
    "            state, done = self.env.reset(), False\n",
    "            equity_curve = []\n",
    "            episode_rewards = []\n",
    "            episode_losses = []\n",
    "            total_steps = 0\n",
    "            max_drawdown = 0\n",
    "            peak_equity = 0\n",
    "            with tqdm(total=len(self.data), desc=f\"Episode {episode + 1}\", leave=False, position=1) as steps_iterator:\n",
    "                while not done:\n",
    "                    action, action_idx = self.select_action(state)\n",
    "                    next_state, reward, real_profit, _, done = self.env.step_forward(action)\n",
    "                    equity_curve.append(real_profit)\n",
    "                    episode_rewards.append(reward)\n",
    "                    total_steps += 1\n",
    "                    peak_equity = max(peak_equity, real_profit)\n",
    "                    current_drawdown = (peak_equity - real_profit) / (peak_equity + 1e-8)\n",
    "                    max_drawdown = max(max_drawdown, current_drawdown)\n",
    "                    state_values = state.to_numpy() if hasattr(state, \"to_numpy\") else np.array(state)\n",
    "                    next_state_values = next_state.to_numpy() if hasattr(next_state, \"to_numpy\") else np.array(next_state)\n",
    "                    self.store_experience(state_values, action_idx, reward, next_state_values, done)\n",
    "                    loss = self.train()\n",
    "                    if loss is not None:\n",
    "                        episode_losses.append(loss.item())\n",
    "                    state = next_state\n",
    "                    steps_iterator.update(1)\n",
    "            self.equity_curves[episode] = equity_curve\n",
    "            episode_summary = {\n",
    "                'episode': episode,\n",
    "                'total_steps': total_steps,\n",
    "                'total_reward': np.sum(episode_rewards),\n",
    "                'mean_reward': np.mean(episode_rewards) if episode_rewards else 0,\n",
    "                'max_drawdown': max_drawdown,\n",
    "                'last_loss': episode_losses[:-1] if episode_losses else None\n",
    "            }\n",
    "            print(episode_summary)\n",
    "            self.update_epsilon()\n",
    "            if self.save_model is not None:\n",
    "                try:\n",
    "                    self.save_model(episode)\n",
    "                except Exception as save_error:\n",
    "                    print(f\"Model save failed in episode {episode}: {save_error}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/processed/train_data_scaled.csv\")\n",
    "\n",
    "\n",
    "agent = TradingAgent(data, plot=False, debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62222f31097041438417b8043cce96d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training Progress:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d7921888ce4c404eb0eaa9e10e1bbff4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Episode 1:   0%|          | 0/1993777 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 119\u001b[39m, in \u001b[36mTradingAgent.run\u001b[39m\u001b[34m(self, episodes)\u001b[39m\n\u001b[32m    117\u001b[39m next_state_values = next_state.to_numpy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(next_state, \u001b[33m\"\u001b[39m\u001b[33mto_numpy\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m np.array(next_state)\n\u001b[32m    118\u001b[39m \u001b[38;5;28mself\u001b[39m.store_experience(state_values, action_idx, reward, next_state_values, done)\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    121\u001b[39m     episode_losses.append(loss.item())\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 69\u001b[39m, in \u001b[36mTradingAgent.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     67\u001b[39m batch = \u001b[38;5;28mself\u001b[39m.sample_experiences()\n\u001b[32m     68\u001b[39m states, actions, rewards, next_states, dones = \u001b[38;5;28mzip\u001b[39m(*batch)\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlong\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdones\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbool\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mQTrainer.train_step\u001b[39m\u001b[34m(self, state, action, reward, next_state, done)\u001b[39m\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Implement Double Q-learning for stability\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m     42\u001b[39m     \u001b[38;5;66;03m# Get actions from main network\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     next_actions = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m.argmax(dim=\u001b[32m1\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# Get Q-values from target network\u001b[39;00m\n\u001b[32m     46\u001b[39m     next_q_values = \u001b[38;5;28mself\u001b[39m.target_model(next_state).gather(\u001b[32m1\u001b[39m, next_actions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\VectorBT_Statistical_Arbitrage\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\VectorBT_Statistical_Arbitrage\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36mLSTM_Q_Net.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(x.shape) == \u001b[32m2\u001b[39m:\n\u001b[32m     31\u001b[39m     x = x.unsqueeze(\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m lstm_out, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m last_step = lstm_out[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m     35\u001b[39m x = F.sigmoid(\u001b[38;5;28mself\u001b[39m.fc1(last_step))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\VectorBT_Statistical_Arbitrage\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\VectorBT_Statistical_Arbitrage\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\VectorBT_Statistical_Arbitrage\\venv\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:1124\u001b[39m, in \u001b[36mLSTM.forward\u001b[39m\u001b[34m(self, input, hx)\u001b[39m\n\u001b[32m   1121\u001b[39m         hx = \u001b[38;5;28mself\u001b[39m.permute_hidden(hx, sorted_indices)\n\u001b[32m   1123\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1124\u001b[39m     result = \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_flat_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[32m   1128\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1129\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1130\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1131\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1132\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbidirectional\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1133\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbatch_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1134\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1135\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1136\u001b[39m     result = _VF.lstm(\n\u001b[32m   1137\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   1138\u001b[39m         batch_sizes,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1145\u001b[39m         \u001b[38;5;28mself\u001b[39m.bidirectional,\n\u001b[32m   1146\u001b[39m     )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "agent.run(50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
