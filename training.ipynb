{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Hyperparameters\n",
    "MAX_MEMORY = 100_000\n",
    "BATCH_SIZE = 128\n",
    "LEARNING_RATE = 0.25\n",
    "GAMMA = 0.95\n",
    "EPSILON_DECAY = 0.99\n",
    "MIN_EPSILON = 0.01\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if device.type == 'cuda':\n",
    "    torch.backends.cudnn.benchmark = True # Optimize CUDA operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from enum import Enum\n",
    "from typing import Tuple, List, Optional\n",
    "\n",
    "class Action(Enum):\n",
    "    HOLD = 0\n",
    "    LONG = 1\n",
    "    SHORT = 2\n",
    "    FLATTEN = 3\n",
    "\n",
    "class TradeSide(Enum):\n",
    "    LONG = 0\n",
    "    SHORT = 1\n",
    "\n",
    "class Position:\n",
    "    def __init__(self, entry_price: float, size: int, side: TradeSide):\n",
    "        self.entry_price = entry_price\n",
    "        self.size = size\n",
    "        self.side = side\n",
    "\n",
    "    def get_unrealized_pnl(self, current_price: float) -> float:\n",
    "        diff = (current_price - self.entry_price) if self.side == TradeSide.LONG else (self.entry_price - current_price)\n",
    "        return self.entry_price * self.size * (diff / ((self.entry_price + current_price) / 2) / 100)\n",
    "\n",
    "    def close(self, current_price: float) -> float:\n",
    "        return self.get_unrealized_pnl(current_price)\n",
    "\n",
    "class TradingEnvironment:\n",
    "    REQUIRED_FEATURES = [\n",
    "        \"Asset1_Price\", \"Asset2_Price\", \"Ratio_Price\", \"Spread_ZScore\", \"Rolling_Correlation\",\n",
    "        \"Rolling_Cointegration_Score\", \"RSI1\", \"RSI2\", \"RSI3\",\n",
    "        \"MACD1\", \"MACD2\", \"MACD3\"\n",
    "    ]\n",
    "\n",
    "    PERFORMANCE_COLUMNS = [\"Unrealized_PnL\", \"Realized_PnL\", \"Positioned\"]\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, plot: bool = False, debug: bool = False):\n",
    "        self.step = 0\n",
    "        self.window_size = 1\n",
    "        self.debug = debug\n",
    "        self.plot = plot\n",
    "        self.balance = self.equity = 1_000_000\n",
    "        self.positions: List[Tuple[Position, Position]] = []\n",
    "        self.realized_pnl = self.unrealized_pnl = 0\n",
    "        self.trade_history, self.reward_history = [], []\n",
    "        self.buy_signals, self.sell_signals = [], []\n",
    "        self.last_trade_step = 0\n",
    "        self._prepare_data(df)\n",
    "\n",
    "    def _prepare_data(self, df: pd.DataFrame):\n",
    "        missing_features = [f for f in self.REQUIRED_FEATURES if f not in df.columns]\n",
    "        if missing_features:\n",
    "            raise ValueError(f\"Missing features: {missing_features}\")\n",
    "\n",
    "        self.data = df.copy()[self.REQUIRED_FEATURES]\n",
    "        for col in self.PERFORMANCE_COLUMNS:\n",
    "            self.data[col] = 0\n",
    "\n",
    "        if \"Hedge_Ratio\" not in df.columns:\n",
    "            raise ValueError(\"Missing 'Hedge_Ratio' column\")\n",
    "\n",
    "        self.hedge_ratio = df[\"Hedge_Ratio\"]\n",
    "\n",
    "    def reset(self) -> pd.Series:\n",
    "        self.step = self.realized_pnl = self.unrealized_pnl = 0\n",
    "        self.balance = self.equity = 1_000_000\n",
    "        self.positions.clear()\n",
    "        self.trade_history.clear()\n",
    "        self.reward_history.clear()\n",
    "        self.buy_signals.clear()\n",
    "        self.sell_signals.clear()\n",
    "        self.last_trade_step = 0\n",
    "        return self.current_observation()\n",
    "\n",
    "    def step_forward(self, action: Action) -> Tuple[pd.Series, float, float, float, bool]:\n",
    "        self.step += 1\n",
    "        self.last_trade_step += 1\n",
    "\n",
    "        reward, profit, positioned = self.execute_trade(action)\n",
    "\n",
    "        self.trade_history.append(self.realized_pnl)\n",
    "        self.reward_history.append(reward)\n",
    "\n",
    "        # Fixed: Added debugging and proper None check\n",
    "        if self.data is None:\n",
    "            if self.debug:\n",
    "              print(\"DEBUG: self.data is None in step_forward\")\n",
    "            done = True\n",
    "        else:\n",
    "            done = self.step + self.window_size >= len(self.data)\n",
    "\n",
    "        # Get a copy of the observation to avoid SettingWithCopyWarning\n",
    "        obs = self.current_observation()\n",
    "\n",
    "        # Update the copy with performance metrics\n",
    "        obs.update({\"Unrealized_PnL\": self.unrealized_pnl, \"Realized_PnL\": self.realized_pnl, \"Positioned\": positioned})\n",
    "\n",
    "        return obs, reward, self.realized_pnl, self.unrealized_pnl, done\n",
    "\n",
    "    def execute_trade(self, action: Action) -> Tuple[float, float, int]:\n",
    "      asset1_price = self.data.iloc[self.step][\"Asset1_Price\"]\n",
    "      asset2_price = self.data.iloc[self.step][\"Asset2_Price\"]\n",
    "      hedge_ratio = self.hedge_ratio.iloc[self.step]\n",
    "\n",
    "      profit = reward = 0\n",
    "\n",
    "      if action == Action.HOLD:\n",
    "          self.update_unrealized_pnl(asset1_price, asset2_price)\n",
    "\n",
    "      elif action in (Action.LONG, Action.SHORT) and not self.positions:\n",
    "          size = 100_000\n",
    "\n",
    "          # Fixed: Added missing closing parenthesis\n",
    "          positions = (\n",
    "              Position(asset1_price, size, TradeSide.SHORT if action == Action.LONG else TradeSide.LONG),\n",
    "              Position(asset2_price, int(size * hedge_ratio), TradeSide.LONG if action == Action.LONG else TradeSide.SHORT)\n",
    "          )  # Added closing parenthesis\n",
    "\n",
    "          self.positions.append(positions)\n",
    "\n",
    "          (self.buy_signals if action == Action.LONG else self.sell_signals).append(self.step)\n",
    "          self.last_trade_step = 0\n",
    "\n",
    "      elif action == Action.FLATTEN and self.positions:\n",
    "          pnl = sum(pos_a.close(asset1_price) + pos_b.close(asset2_price) for pos_a, pos_b in self.positions)\n",
    "          self.realized_pnl += pnl\n",
    "          self.unrealized_pnl = 0\n",
    "          self.positions.clear()\n",
    "\n",
    "          profit = pnl\n",
    "          reward = self.calculate_reward(pnl)  # This might return None if not implemented\n",
    "          if reward is None:\n",
    "              print(\"DEBUG: calculate_reward returned None\")\n",
    "              reward = pnl  # Default to using pnl as reward\n",
    "\n",
    "      return reward, profit, int(bool(self.positions))\n",
    "\n",
    "    def update_unrealized_pnl(self, asset1_price: float, asset2_price: float):\n",
    "      # Fixed: Added closing parenthesis and None check\n",
    "      if not self.positions:\n",
    "          self.unrealized_pnl = 0\n",
    "          return\n",
    "\n",
    "      self.unrealized_pnl = sum(\n",
    "          pos_a.get_unrealized_pnl(asset1_price) + pos_b.get_unrealized_pnl(asset2_price)\n",
    "          for pos_a, pos_b in self.positions\n",
    "      )  # Added closing parenthesis\n",
    "\n",
    "    def calculate_reward(self, pnl: float):\n",
    "      \"\"\"\n",
    "      Calculate reward based on PnL with proper return value.\n",
    "      \"\"\"\n",
    "      # Implement a basic reward function if it's not already defined\n",
    "      if pnl is None:\n",
    "          return 0.0\n",
    "      return float(pnl)  # Ensure we return a float value\n",
    "\n",
    "    def current_observation(self) -> pd.Series:\n",
    "        # Fixed: Added error handling and debug output\n",
    "        if self.data is None:\n",
    "            print(\"DEBUG: self.data is None in current_observation\")\n",
    "            # Return empty Series with required columns\n",
    "            return pd.Series({feature: 0 for feature in self.REQUIRED_FEATURES + self.PERFORMANCE_COLUMNS})\n",
    "        elif self.step >= len(self.data):\n",
    "            print(f\"DEBUG: Step index {self.step} is out of bounds for data length {len(self.data)}\")\n",
    "            # Return last valid observation\n",
    "            return self.data.iloc[-1].copy()\n",
    "        else:\n",
    "            # Return a copy instead of a view to avoid the SettingWithCopyWarning\n",
    "            return self.data.iloc[self.step].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional a5s F\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "class LSTM_Q_Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, dropout=0.3, bidirectional=False):\n",
    "        super(LSTM_Q_Net, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Fixed: Added missing closing parenthesis\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=2, # Reduced from 3 for faster processing\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional,\n",
    "        )  # Added closing parenthesis here\n",
    "\n",
    "        mult = 2 if bidirectional else 1\n",
    "        self.ln = nn.LayerNorm(hidden_size * mult)\n",
    "        self.fc1 = nn.Linear(hidden_size * mult, 128)\n",
    "        self.fc2 = nn.Linear(128, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle both single samples and batches efficiently\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.unsqueeze(1) # Add sequence dimension if missing\n",
    "\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "\n",
    "        # Use the last timestep output\n",
    "        last_step = lstm_out[:, -1, :]\n",
    "        normalized = self.ln(last_step)\n",
    "\n",
    "        # Fully connected layers\n",
    "        x = F.relu(self.fc1(normalized))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def save(self, file_name):\n",
    "        model_folder_path = \"./models\"\n",
    "        os.makedirs(model_folder_path, exist_ok=True)\n",
    "        file_path = os.path.join(model_folder_path, file_name)\n",
    "        torch.save(self.state_dict(), file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTrainer:\n",
    "    def __init__(self, model, lr, gamma, batch_size, target_update_freq=50):\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.model = model.to(device)\n",
    "\n",
    "        # Create target network with same architecture\n",
    "        self.target_model = LSTM_Q_Net(\n",
    "            input_size=15,\n",
    "            hidden_size=model.hidden_size,\n",
    "            output_size=model.fc2.out_features,\n",
    "            bidirectional=model.bidirectional\n",
    "        ).to(device)\n",
    "\n",
    "        self.update_target()\n",
    "\n",
    "        # Use Adam with improved parameters\n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=self.lr, amsgrad=True)\n",
    "\n",
    "        # Less frequent LR adjustments\n",
    "        self.scheduler = optim.lr_scheduler.StepLR(self.optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "        # Huber loss (SmoothL1Loss) is more robust for RL\n",
    "        self.criterion = nn.SmoothL1Loss()\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update_freq = target_update_freq\n",
    "        self.train_step_count = 0\n",
    "\n",
    "    def update_target(self):\n",
    "        self.target_model.load_state_dict(self.model.state_dict())\n",
    "\n",
    "    def train_step(self, state, action, reward, next_state, done):\n",
    "        # Reshape action for gathering\n",
    "        action = action.view(-1, 1)\n",
    "\n",
    "        # Get current Q values\n",
    "        pred = self.model(state)\n",
    "\n",
    "        # Implement Double Q-learning for stability\n",
    "        with torch.no_grad():\n",
    "            # Get actions from main network\n",
    "            next_actions = self.model(next_state).argmax(dim=1, keepdim=True)\n",
    "\n",
    "            # Get Q-values from target network\n",
    "            next_q_values = self.target_model(next_state).gather(1, next_actions)\n",
    "\n",
    "            # Compute target Q values\n",
    "            target = reward.unsqueeze(1) + (1 - done.float().unsqueeze(1)) * self.gamma * next_q_values\n",
    "\n",
    "        # Get Q values for taken actions\n",
    "        q_values = pred.gather(1, action)\n",
    "\n",
    "        # Calculate loss and optimize\n",
    "        loss = self.criterion(q_values, target)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Update target network less frequently\n",
    "        self.train_step_count += 1\n",
    "        if self.train_step_count % self.target_update_freq == 0:\n",
    "            self.update_target()\n",
    "            self.scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Training Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TradingAgent:\n",
    "    def __init__(self, data: pd.DataFrame, plot: bool = False, debug: bool = False):\n",
    "        self.debug = debug\n",
    "        self.epsilon = 0.25\n",
    "        self.memory = deque(maxlen=MAX_MEMORY)\n",
    "        self.model = LSTM_Q_Net(input_size=15, hidden_size=128, output_size=4).to(device)\n",
    "        self.trainer = QTrainer(self.model, lr=LEARNING_RATE, gamma=GAMMA, batch_size=BATCH_SIZE)\n",
    "        self.env = TradingEnvironment(data, plot, debug)\n",
    "        self.length = len(data)\n",
    "        self.equity_curves = {}\n",
    "\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "      if reward is None or np.isnan(reward):  # Handle None or NaN rewards\n",
    "          print(\"DEBUG: Received None or NaN reward, setting to 0.0\")\n",
    "          reward = 0.0\n",
    "      else:\n",
    "          reward = np.clip(reward, -1, 1)\n",
    "\n",
    "      self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "\n",
    "    def sample_experiences(self):\n",
    "        if len(self.memory) < BATCH_SIZE:\n",
    "            return list(self.memory)  # Return whatever is available\n",
    "\n",
    "        priorities = np.abs(np.array([exp[2] for exp in self.memory]))  # Absolute rewards\n",
    "        sum_priorities = np.sum(priorities)\n",
    "\n",
    "        # Ensure priorities are non-zero by adding a small constant\n",
    "        if sum_priorities == 0:\n",
    "            probabilities = np.ones(len(self.memory)) / len(self.memory)  # Uniform sampling\n",
    "        else:\n",
    "            probabilities = (priorities + 1e-6) / (sum_priorities + 1e-6)  # Avoid division by zero\n",
    "\n",
    "        # FIX: Normalize probabilities to ensure they sum to exactly 1\n",
    "        probabilities = probabilities / np.sum(probabilities)\n",
    "\n",
    "        sample_size = min(BATCH_SIZE, len(self.memory))\n",
    "        indices = np.random.choice(len(self.memory), sample_size, p=probabilities, replace=False)\n",
    "\n",
    "        return [self.memory[i] for i in indices]\n",
    "\n",
    "    def update_epsilon(self):\n",
    "        self.epsilon = max(MIN_EPSILON, self.epsilon * EPSILON_DECAY)\n",
    "\n",
    "    @torch.no_grad() # Disable gradient tracking for inference\n",
    "    def select_action(self, state: pd.Series) -> Tuple[Action, int]:\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.random() < self.epsilon:\n",
    "            action_idx = np.random.randint(0, 4)\n",
    "        else:\n",
    "            # Single conversion to tensor with proper handling of NaN values\n",
    "            state_tensor = torch.tensor(np.nan_to_num(state.values),\n",
    "                                      dtype=torch.float32).unsqueeze(0).to(device)\n",
    "            action_idx = torch.argmax(self.model(state_tensor)).item()\n",
    "\n",
    "        return [Action.HOLD, Action.LONG, Action.SHORT, Action.FLATTEN][action_idx], action_idx\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"\n",
    "        Trains the model using a mini-batch from experience replay.\n",
    "        \"\"\"\n",
    "        batch = self.sample_experiences()\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        self.trainer.train_step(\n",
    "            torch.tensor(np.array(states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(actions, dtype=torch.long).to(device),\n",
    "            torch.tensor(rewards, dtype=torch.float32).to(device),\n",
    "            torch.tensor(np.array(next_states), dtype=torch.float32).to(device),\n",
    "            torch.tensor(dones, dtype=torch.bool).to(device)\n",
    "        )  # Fixed: Added missing closing parenthesis\n",
    "\n",
    "    def save_model(self, episode: int):\n",
    "      \"\"\"\n",
    "      Saves the trained model with proper error handling.\n",
    "      \"\"\"\n",
    "      try:\n",
    "          if episode is not None:\n",
    "              print(f\"DEBUG: Saving model for episode {episode}\")\n",
    "              self.model.save(f\"Episode-{episode}.pth\")\n",
    "          else:\n",
    "              print(\"DEBUG: Episode variable is None, using fallback name\")\n",
    "              self.model.save(\"Episode-unknown.pth\")\n",
    "      except Exception as e:\n",
    "          print(f\"DEBUG: Error in save_model - {str(e)}\")\n",
    "\n",
    "    def run(self, episodes: int):\n",
    "        progress = tqdm(total=self.length)\n",
    "        for episode in range(episodes):\n",
    "            state, done = self.env.reset(), False\n",
    "            equity_curve = []\n",
    "            while not done:\n",
    "                progress.update(1)\n",
    "                action, action_idx = self.select_action(state)\n",
    "                next_state, reward, real_profit, _, done = self.env.step_forward(action)\n",
    "                equity_curve.append(real_profit)\n",
    "\n",
    "                # Store experience using NumPy arrays directly\n",
    "                self.store_experience(state.values, action_idx, reward, next_state.values, done)\n",
    "                self.train()\n",
    "                state = next_state\n",
    "\n",
    "            self.equity_curves[episode] = equity_curve\n",
    "            self.update_epsilon()\n",
    "            self.save_model(episode)  # Now properly handles None case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed')\n",
    "\n",
    "agent = TradingAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Training Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
